---
title: "The motivation: Why even border to learn Machine Learning?"
last_modified_at: 2019-11-01T18:00:02+02:00
read_time: false
categories:
  - Publications
header:
  teaser: assets/blog/2019_11_01/mika-baumeister-Wpnoqo2plFA-unsplash.jpg
---


This post provides a first overview of emerging approaches for ML (machine learning) and the motivation behind learning ML.


## ML is all around us. Starting with

Stock market transactions :chart_with_upwards_trend:, Credit applications :credit_card:, “Counterterrorism” / Mass surveillance  
:mag_right:, Dating websites (e.g., hinge, tinder) :couple_with_heart:, Natural language processing (NLP) :speech_balloon:, Navigation systems :oncoming_automobile: and Medical applications :pill: 

![img](/assets/blog/2019_11_01/5Q5OC.png){: .align-center}

In the least complex sense, ML is a method of computer data analysis that learns from its own "experience". Once a machine learning algorithm recognizes what specific patterns look like, it can apply the knowledge on a vast scale.

During the last decades, there has been great progress in ML. NLP (Natural Language Processing), a category of ML, has made substantial [progress](https://ruder.io/tracking-progress-nlp/) in the past years.
Only five years ago the performance in key tasks stagnated and gaining [1pp](https://en.wikipedia.org/wiki/PP_(complexity)) was considered a major success. Today almost all tasks gain +10pp. This is due to deep learning. The training of deep learning, that is a category of ML, is based on word/sentence/character embeddings. Its models are collections of high dimensional vector spaces and there is no interpretation possible. It's a 'black box'.

## But what is a black box anyway? 

A Black Box Model is a framework that doesn't uncover its internal mechanisms. In ML, the "black box" portrays models that can't be comprehended by taking a look at their parameters (for example a neural network). Something contrary to a black box is some of the time alluded to as White Box.
It's important to know the mechanisms that are inside an ML model, this is where those algorithms and their engineers need to become accountable. Because humans transfer their bias to algorithms, they can create injustice, that's one reason why we need to hold algorithms and their developers accountable. This is where Interpretable Machine LEarning can help. But more on this later...

![img](/assets/blog/2019_11_01/P1WaO.png){: .align-center}

The most important thing why you should learn ML is that you get a whole new world of new research questions for your topic. It doesn't matter if you are a social scientist, historian, linguist, data journalist, etc.  Let's see where this will take us during the tutorial.
Check out the next page to find out about the essential terminology and what all those terms mean.

## Replacing one machine we don’t understand with another one we don’t understand doesn’t help us
D. Gerstorfer
